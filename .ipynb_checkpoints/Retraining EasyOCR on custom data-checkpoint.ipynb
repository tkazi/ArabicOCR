{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a4090294-3349-4815-96f4-98010b657359",
     "kernelId": ""
    }
   },
   "source": [
    "# Retraining EasyOCR on custom data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------- Loading Libraries --------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.711614Z",
     "iopub.status.busy": "2023-05-02T18:01:13.711089Z",
     "iopub.status.idle": "2023-05-02T18:01:13.720568Z",
     "shell.execute_reply": "2023-05-02T18:01:13.719277Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.711566Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch._utils import _accumulate\n",
    "import torch\n",
    "import re\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import string\n",
    "import argparse\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import sys\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "import six\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------ Training Code --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.723160Z",
     "iopub.status.busy": "2023-05-02T18:01:13.722741Z",
     "iopub.status.idle": "2023-05-02T18:01:13.740417Z",
     "shell.execute_reply": "2023-05-02T18:01:13.739228Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.723114Z"
    }
   },
   "outputs": [],
   "source": [
    "class CTCLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    #def __init__(self, character, separator = []):\n",
    "    def __init__(self, character, separator_list = {}, dict_pathlist = {}):\n",
    "        # character (str): set of the possible characters.\n",
    "        dict_character = list(character)\n",
    "\n",
    "        #special_character = ['\\xa2', '\\xa3', '\\xa4','\\xa5']\n",
    "        #self.separator_char = special_character[:len(separator)]\n",
    "\n",
    "        self.dict = {}\n",
    "        #for i, char in enumerate(self.separator_char + dict_character):\n",
    "        for i, char in enumerate(dict_character):\n",
    "            # NOTE: 0 is reserved for 'blank' token required by CTCLoss\n",
    "            self.dict[char] = i + 1\n",
    "\n",
    "        self.character = ['[blank]'] + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
    "        #self.character = ['[blank]']+ self.separator_char + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
    "        self.separator_list = separator_list\n",
    "\n",
    "        separator_char = []\n",
    "        for lang, sep in separator_list.items():\n",
    "            separator_char += sep\n",
    "\n",
    "        self.ignore_idx = [0] + [i+1 for i,item in enumerate(separator_char)]\n",
    "\n",
    "        dict_list = {}\n",
    "        for lang, dict_path in dict_pathlist.items():\n",
    "            with open(dict_path, \"rb\") as input_file:\n",
    "                word_count = pickle.load(input_file)\n",
    "            dict_list[lang] = word_count\n",
    "        self.dict_list = dict_list #checkpoint\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\"convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "        output:\n",
    "            text: concatenated text index for CTCLoss.\n",
    "                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n",
    "            length: length of each text. [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) for s in text]\n",
    "        text = ''.join(text)\n",
    "        text = [self.dict[char] for char in text]\n",
    "\n",
    "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
    "\n",
    "    def decode_greedy(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        index = 0\n",
    "        for l in length:\n",
    "            t = text_index[index:index + l]\n",
    "\n",
    "            char_list = []\n",
    "            for i in range(l):\n",
    "                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
    "                #if (t[i] != 0) and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
    "                    char_list.append(self.character[t[i]])\n",
    "            text = ''.join(char_list)\n",
    "\n",
    "            texts.append(text)\n",
    "            index += l\n",
    "        return texts\n",
    "\n",
    "    def decode_beamsearch(self, mat, beamWidth=5):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(mat.shape[0]):\n",
    "            t = ctcBeamSearch(mat[i], self.character, self.ignore_idx, None, beamWidth=beamWidth)\n",
    "            texts.append(t)\n",
    "        return texts\n",
    "\n",
    "    def decode_wordbeamsearch(self, mat, beamWidth=5):\n",
    "        texts = []\n",
    "        argmax = np.argmax(mat, axis = 2)\n",
    "        for i in range(mat.shape[0]):\n",
    "            words = word_segmentation(argmax[i])\n",
    "            string = ''\n",
    "            for word in words:\n",
    "                matrix = mat[i, word[1][0]:word[1][1]+1,:]\n",
    "                if word[0] == '': dict_list = []\n",
    "                else: dict_list = self.dict_list[word[0]]\n",
    "                t = ctcBeamSearch(matrix, self.character, self.ignore_idx, None, beamWidth=beamWidth, dict_list=dict_list)\n",
    "                string += t\n",
    "            texts.append(string)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.742444Z",
     "iopub.status.busy": "2023-05-02T18:01:13.742110Z",
     "iopub.status.idle": "2023-05-02T18:01:13.750923Z",
     "shell.execute_reply": "2023-05-02T18:01:13.750020Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.742417Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttnLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    def __init__(self, character):\n",
    "        # character (str): set of the possible characters.\n",
    "        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.\n",
    "        list_token = ['[GO]', '[s]']  # ['[s]','[UNK]','[PAD]','[GO]']\n",
    "        list_character = list(character)\n",
    "        self.character = list_token + list_character\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(self.character):\n",
    "            # print(i, char)\n",
    "            self.dict[char] = i\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\" convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "            batch_max_length: max length of text label in the batch. 25 by default\n",
    "        output:\n",
    "            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.\n",
    "                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.\n",
    "            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.\n",
    "        # batch_max_length = max(length) # this is not allowed for multi-gpu setting\n",
    "        batch_max_length += 1\n",
    "        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.\n",
    "        batch_text = torch.LongTensor(len(text), batch_max_length + 1).fill_(0)\n",
    "        for i, t in enumerate(text):\n",
    "            text = list(t)\n",
    "            text.append('[s]')\n",
    "            text = [self.dict[char] for char in text]\n",
    "            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token\n",
    "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
    "\n",
    "    def decode(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        for index, l in enumerate(length):\n",
    "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.752195Z",
     "iopub.status.busy": "2023-05-02T18:01:13.751916Z",
     "iopub.status.idle": "2023-05-02T18:01:13.757824Z",
     "shell.execute_reply": "2023-05-02T18:01:13.757003Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.752160Z"
    }
   },
   "outputs": [],
   "source": [
    "class Averager(object):\n",
    "    \"\"\"Compute average for torch.Tensor, used for loss average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def add(self, v):\n",
    "        count = v.data.numel()\n",
    "        v = v.data.sum()\n",
    "        self.n_count += count\n",
    "        self.sum += v\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_count = 0\n",
    "        self.sum = 0\n",
    "\n",
    "    def val(self):\n",
    "        res = 0\n",
    "        if self.n_count != 0:\n",
    "            res = self.sum / float(self.n_count)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.760823Z",
     "iopub.status.busy": "2023-05-02T18:01:13.760571Z",
     "iopub.status.idle": "2023-05-02T18:01:13.767676Z",
     "shell.execute_reply": "2023-05-02T18:01:13.766388Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.760800Z"
    }
   },
   "outputs": [],
   "source": [
    "def hierarchical_dataset(root, opt, select_data='/'):\n",
    "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
    "    dataset_list = []\n",
    "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
    "    print(dataset_log)\n",
    "    dataset_log += '\\n'\n",
    "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
    "        if not dirnames:\n",
    "            select_flag = False\n",
    "            for selected_d in select_data:\n",
    "                if selected_d in dirpath:\n",
    "                    select_flag = True\n",
    "                    break\n",
    "\n",
    "            if select_flag:\n",
    "                dataset = OCRDataset(dirpath, opt)\n",
    "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
    "                print(sub_dataset_log)\n",
    "                dataset_log += f'{sub_dataset_log}\\n'\n",
    "                dataset_list.append(dataset)\n",
    "\n",
    "    concatenated_dataset = ConcatDataset(dataset_list)\n",
    "\n",
    "    return concatenated_dataset, dataset_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.769241Z",
     "iopub.status.busy": "2023-05-02T18:01:13.768985Z",
     "iopub.status.idle": "2023-05-02T18:01:13.780354Z",
     "shell.execute_reply": "2023-05-02T18:01:13.779348Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.769216Z"
    }
   },
   "outputs": [],
   "source": [
    "class AlignCollate(object):\n",
    "\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False, contrast_adjust = 0.):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "        self.contrast_adjust = contrast_adjust\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        images, labels = zip(*batch)\n",
    "\n",
    "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
    "            resized_max_w = self.imgW\n",
    "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
    "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "            resized_images = []\n",
    "            for image in images:\n",
    "                w, h = image.size\n",
    "\n",
    "                #### augmentation here - change contrast\n",
    "                if self.contrast_adjust > 0:\n",
    "                    image = np.array(image.convert(\"L\"))\n",
    "                    image = adjust_contrast_grey(image, target = self.contrast_adjust)\n",
    "                    image = Image.fromarray(image, 'L')\n",
    "\n",
    "                ratio = w / float(h)\n",
    "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                    resized_w = self.imgW\n",
    "                else:\n",
    "                    resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "                resized_image = image.resize((resized_w, self.imgH), Image.Resampling.BICUBIC)\n",
    "                resized_images.append(transform(resized_image))\n",
    "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
    "\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
    "\n",
    "        else:\n",
    "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
    "            image_tensors = [transform(image) for image in images]\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
    "\n",
    "        return image_tensors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.782082Z",
     "iopub.status.busy": "2023-05-02T18:01:13.781796Z",
     "iopub.status.idle": "2023-05-02T18:01:13.796500Z",
     "shell.execute_reply": "2023-05-02T18:01:13.795397Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.782056Z"
    }
   },
   "outputs": [],
   "source": [
    "class Batch_Balanced_Dataset(object):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"\n",
    "        Modulate the data ratio in the batch.\n",
    "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
    "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
    "        \"\"\"\n",
    "        log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a')\n",
    "        dashed_line = '-' * 80\n",
    "        print(dashed_line)\n",
    "        log.write(dashed_line + '\\n')\n",
    "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
    "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
    "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
    "\n",
    "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust = opt.contrast_adjust)\n",
    "        self.data_loader_list = []\n",
    "        self.dataloader_iter_list = []\n",
    "        batch_size_list = []\n",
    "        Total_batch_size = 0\n",
    "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
    "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
    "            print(dashed_line)\n",
    "            log.write(dashed_line + '\\n')\n",
    "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
    "            total_number_dataset = len(_dataset)\n",
    "            log.write(_dataset_log)\n",
    "\n",
    "            \"\"\"\n",
    "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
    "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
    "            See 4.2 section in our paper.\n",
    "            \"\"\"\n",
    "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
    "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
    "            indices = range(total_number_dataset)\n",
    "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
    "                           for offset, length in zip(_accumulate(dataset_split), dataset_split)]\n",
    "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
    "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
    "            print(selected_d_log)\n",
    "            log.write(selected_d_log + '\\n')\n",
    "            batch_size_list.append(str(_batch_size))\n",
    "            Total_batch_size += _batch_size\n",
    "\n",
    "            _data_loader = torch.utils.data.DataLoader(\n",
    "                _dataset, batch_size=_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=int(opt.workers), #prefetch_factor=2,persistent_workers=True,\n",
    "                collate_fn=_AlignCollate, pin_memory=True)\n",
    "            self.data_loader_list.append(_data_loader)\n",
    "            self.dataloader_iter_list.append(iter(_data_loader))\n",
    "\n",
    "        Total_batch_size_log = f'{dashed_line}\\n'\n",
    "        batch_size_sum = '+'.join(batch_size_list)\n",
    "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
    "        Total_batch_size_log += f'{dashed_line}'\n",
    "        opt.batch_size = Total_batch_size\n",
    "\n",
    "        print(Total_batch_size_log)\n",
    "        log.write(Total_batch_size_log + '\\n')\n",
    "        log.close()\n",
    "\n",
    "    def get_batch(self):\n",
    "        balanced_batch_images = []\n",
    "        balanced_batch_texts = []\n",
    "\n",
    "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
    "            try:\n",
    "\n",
    "                data_loader_iter = iter(data_loader_iter)\n",
    "\n",
    "                image, text = next(data_loader_iter)#.next()\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except StopIteration:\n",
    "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
    "                image, text = next(self.dataloader_iter_list[i])#.next()\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
    "\n",
    "        return balanced_batch_images, balanced_batch_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.797869Z",
     "iopub.status.busy": "2023-05-02T18:01:13.797632Z",
     "iopub.status.idle": "2023-05-02T18:01:13.811577Z",
     "shell.execute_reply": "2023-05-02T18:01:13.810410Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.797844Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        super(Model, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.stages = {'Trans': opt.Transformation, 'Feat': opt.FeatureExtraction,\n",
    "                       'Seq': opt.SequenceModeling, 'Pred': opt.Prediction}\n",
    "\n",
    "        \"\"\" Transformation \"\"\"\n",
    "        if opt.Transformation == 'TPS':\n",
    "            self.Transformation = TPS_SpatialTransformerNetwork(\n",
    "                F=opt.num_fiducial, I_size=(opt.imgH, opt.imgW), I_r_size=(opt.imgH, opt.imgW), I_channel_num=opt.input_channel)\n",
    "        else:\n",
    "            print('No Transformation module specified')\n",
    "\n",
    "        \"\"\" FeatureExtraction \"\"\"\n",
    "        if opt.FeatureExtraction == 'VGG':\n",
    "            self.FeatureExtraction = VGG_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        elif opt.FeatureExtraction == 'RCNN':\n",
    "            self.FeatureExtraction = RCNN_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        elif opt.FeatureExtraction == 'ResNet':\n",
    "            self.FeatureExtraction = ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        else:\n",
    "            raise Exception('No FeatureExtraction module specified')\n",
    "        self.FeatureExtraction_output = opt.output_channel  # int(imgH/16-1) * 512\n",
    "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1\n",
    "\n",
    "        \"\"\" Sequence modeling\"\"\"\n",
    "        if opt.SequenceModeling == 'BiLSTM':\n",
    "            self.SequenceModeling = nn.Sequential(\n",
    "                BidirectionalLSTM(self.FeatureExtraction_output, opt.hidden_size, opt.hidden_size),\n",
    "                BidirectionalLSTM(opt.hidden_size, opt.hidden_size, opt.hidden_size))\n",
    "            self.SequenceModeling_output = opt.hidden_size\n",
    "        else:\n",
    "            print('No SequenceModeling module specified')\n",
    "            self.SequenceModeling_output = self.FeatureExtraction_output\n",
    "\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        if opt.Prediction == 'CTC':\n",
    "            self.Prediction = nn.Linear(self.SequenceModeling_output, opt.num_class)\n",
    "        elif opt.Prediction == 'Attn':\n",
    "            self.Prediction = Attention(self.SequenceModeling_output, opt.hidden_size, opt.num_class)\n",
    "        else:\n",
    "            raise Exception('Prediction is neither CTC or Attn')\n",
    "\n",
    "    def forward(self, input, text, is_train=True):\n",
    "        \"\"\" Transformation stage \"\"\"\n",
    "        if not self.stages['Trans'] == \"None\":\n",
    "            input = self.Transformation(input)\n",
    "\n",
    "        \"\"\" Feature extraction stage \"\"\"\n",
    "        visual_feature = self.FeatureExtraction(input)\n",
    "        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
    "        visual_feature = visual_feature.squeeze(3)\n",
    "\n",
    "        \"\"\" Sequence modeling stage \"\"\"\n",
    "        if self.stages['Seq'] == 'BiLSTM':\n",
    "            contextual_feature = self.SequenceModeling(visual_feature)\n",
    "        else:\n",
    "            contextual_feature = visual_feature  # for convenience. this is NOT contextually modeled by BiLSTM\n",
    "\n",
    "        \"\"\" Prediction stage \"\"\"\n",
    "        if self.stages['Pred'] == 'CTC':\n",
    "            prediction = self.Prediction(contextual_feature.contiguous())\n",
    "        else:\n",
    "            prediction = self.Prediction(contextual_feature.contiguous(), text, is_train, batch_max_length=self.opt.batch_max_length)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.813324Z",
     "iopub.status.busy": "2023-05-02T18:01:13.813078Z",
     "iopub.status.idle": "2023-05-02T18:01:13.828063Z",
     "shell.execute_reply": "2023-05-02T18:01:13.827275Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.813299Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, criterion, evaluation_loader, converter, opt, device):\n",
    "    \"\"\" validation or evaluation \"\"\"\n",
    "    n_correct = 0\n",
    "    norm_ED = 0\n",
    "    length_of_data = 0\n",
    "    infer_time = 0\n",
    "    valid_loss_avg = Averager()\n",
    "\n",
    "    for i, (image_tensors, labels) in enumerate(evaluation_loader):\n",
    "        batch_size = image_tensors.size(0)\n",
    "        length_of_data = length_of_data + batch_size\n",
    "        image = image_tensors.to(device)\n",
    "        # For max length prediction\n",
    "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "        text_for_loss, length_for_loss = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        if 'CTC' in opt.Prediction:\n",
    "            preds = model(image, text_for_pred)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            # Calculate evaluation loss for CTC decoder.\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "            # permute 'preds' to use CTCloss format\n",
    "            cost = criterion(preds.log_softmax(2).permute(1, 0, 2), text_for_loss, preds_size, length_for_loss)\n",
    "\n",
    "            if opt.decode == 'greedy':\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
    "            elif opt.decode == 'beamsearch':\n",
    "                preds_str = converter.decode_beamsearch(preds, beamWidth=2)\n",
    "\n",
    "        else:\n",
    "            preds = model(image, text_for_pred, is_train=False)\n",
    "            forward_time = time.time() - start_time\n",
    "\n",
    "            preds = preds[:, :text_for_loss.shape[1] - 1, :]\n",
    "            target = text_for_loss[:, 1:]  # without [GO] Symbol\n",
    "            cost = criterion(preds.contiguous().view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "\n",
    "            # select max probabilty (greedy decoding) then decode index to character\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_str = converter.decode(preds_index, length_for_pred)\n",
    "            labels = converter.decode(text_for_loss[:, 1:], length_for_loss)\n",
    "\n",
    "        infer_time += forward_time\n",
    "        valid_loss_avg.add(cost)\n",
    "\n",
    "        # calculate accuracy & confidence score\n",
    "        preds_prob = F.softmax(preds, dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "        confidence_score_list = []\n",
    "        \n",
    "        for gt, pred, pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
    "            if 'Attn' in opt.Prediction:\n",
    "                gt = gt[:gt.find('[s]')]\n",
    "                pred_EOS = pred.find('[s]')\n",
    "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "            if pred == gt:\n",
    "                n_correct += 1\n",
    "\n",
    "            '''\n",
    "            (old version) ICDAR2017 DOST Normalized Edit Distance https://rrc.cvc.uab.es/?ch=7&com=tasks\n",
    "            \"For each word we calculate the normalized edit distance to the length of the ground truth transcription.\" \n",
    "            if len(gt) == 0:\n",
    "                norm_ED += 1\n",
    "            else:\n",
    "                norm_ED += edit_distance(pred, gt) / len(gt)\n",
    "            '''\n",
    "            \n",
    "            # ICDAR2019 Normalized Edit Distance \n",
    "            if len(gt) == 0 or len(pred) ==0:\n",
    "                norm_ED += 0\n",
    "            elif len(gt) > len(pred):\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
    "            else:\n",
    "                norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
    "\n",
    "            # calculate confidence score (= multiply of pred_max_prob)\n",
    "            try:\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "            except:\n",
    "                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([s])\n",
    "            confidence_score_list.append(confidence_score)\n",
    "            # print(pred, gt, pred==gt, confidence_score)\n",
    "\n",
    "    accuracy = n_correct / float(length_of_data) * 100\n",
    "    norm_ED = norm_ED / float(length_of_data) # ICDAR2019 Normalized Edit Distance\n",
    "\n",
    "    return valid_loss_avg.val(), accuracy, norm_ED, preds_str, confidence_score_list, labels, infer_time, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.829311Z",
     "iopub.status.busy": "2023-05-02T18:01:13.829025Z",
     "iopub.status.idle": "2023-05-02T18:01:13.833854Z",
     "shell.execute_reply": "2023-05-02T18:01:13.833081Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.829275Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.834985Z",
     "iopub.status.busy": "2023-05-02T18:01:13.834753Z",
     "iopub.status.idle": "2023-05-02T18:01:13.842732Z",
     "shell.execute_reply": "2023-05-02T18:01:13.841834Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.834963Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_config(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = AttrDict(opt)\n",
    "    if opt.lang_char == 'None':\n",
    "        characters = ''\n",
    "        for data in opt['select_data'].split('-'):\n",
    "            print('check data content: ', data)\n",
    "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
    "            #csv_path = 'all_data/en_train_filtered/labels.csv'\n",
    "            #print('csv_path: ', csv_path)\n",
    "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            #df = pd.read_csv('all_data/en_train_filtered/labels.csv', sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "            all_char = ''.join(df['words'])\n",
    "            characters += ''.join(set(all_char))\n",
    "        characters = sorted(set(characters))\n",
    "        opt.character= ''.join(characters)\n",
    "    else:\n",
    "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
    "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.844429Z",
     "iopub.status.busy": "2023-05-02T18:01:13.844062Z",
     "iopub.status.idle": "2023-05-02T18:01:13.851403Z",
     "shell.execute_reply": "2023-05-02T18:01:13.850266Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.844401Z"
    }
   },
   "outputs": [],
   "source": [
    "class NormalizePAD(object):\n",
    "\n",
    "    def __init__(self, max_size, PAD_type='right'):\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.max_size = max_size\n",
    "        self.max_width_half = math.floor(max_size[2] / 2)\n",
    "        self.PAD_type = PAD_type\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        c, h, w = img.size()\n",
    "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
    "        Pad_img[:, :, :w] = img  # right pad\n",
    "        if self.max_size[2] != w:  # add border Pad\n",
    "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
    "\n",
    "        return Pad_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.852894Z",
     "iopub.status.busy": "2023-05-02T18:01:13.852661Z",
     "iopub.status.idle": "2023-05-02T18:01:13.863693Z",
     "shell.execute_reply": "2023-05-02T18:01:13.862505Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.852870Z"
    }
   },
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, opt):\n",
    "\n",
    "        self.root = root\n",
    "        self.opt = opt\n",
    "        #root = glob.glob(\"all_data/en_train_filtered/[!.]**.csv\")\n",
    "        #root = root[0]\n",
    "        #print('root: ',root)\n",
    "        self.df = pd.read_csv(os.path.join(root,'labels.csv'), sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "        #self.df = pd.read_csv(root, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
    "        self.nSamples = len(self.df)\n",
    "\n",
    "        if self.opt.data_filtering_off:\n",
    "            self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
    "        else:\n",
    "            self.filtered_index_list = []\n",
    "            for index in range(self.nSamples):\n",
    "                label = self.df.at[index,'words']\n",
    "                try:\n",
    "                    if len(label) > self.opt.batch_max_length:\n",
    "                        continue\n",
    "                except:\n",
    "                    print(label)\n",
    "                out_of_char = f'[^{self.opt.character}]'\n",
    "                #label = str(label).lower()\n",
    "                #if re.search(out_of_char, label.lower()):\n",
    "                if re.search(out_of_char, str(label).lower()):\n",
    "                    continue\n",
    "                self.filtered_index_list.append(index)\n",
    "            self.nSamples = len(self.filtered_index_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.filtered_index_list[index]\n",
    "        img_fname = self.df.at[index,'filename']\n",
    "        img_fpath = os.path.join(self.root, img_fname)\n",
    "        label = self.df.at[index,'words']\n",
    "\n",
    "        if self.opt.rgb:\n",
    "            img = Image.open(img_fpath).convert('RGB')  # for color image\n",
    "        else:\n",
    "            img = Image.open(img_fpath).convert('L')\n",
    "\n",
    "        if not self.opt.sensitive:\n",
    "            label = label.lower()\n",
    "\n",
    "        # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
    "        out_of_char = f'[^{self.opt.character}]'\n",
    "        label = str(label).lower()\n",
    "        label = re.sub(out_of_char, '', label)\n",
    "        \n",
    "\n",
    "        return (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.867702Z",
     "iopub.status.busy": "2023-05-02T18:01:13.867452Z",
     "iopub.status.idle": "2023-05-02T18:01:13.877754Z",
     "shell.execute_reply": "2023-05-02T18:01:13.876706Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.867678Z"
    }
   },
   "outputs": [],
   "source": [
    "class VGG_FeatureExtractor(nn.Module):\n",
    "    \"\"\" FeatureExtractor of CRNN (https://arxiv.org/pdf/1507.05717.pdf) \"\"\"\n",
    "\n",
    "    def __init__(self, input_channel, output_channel=512):\n",
    "        super(VGG_FeatureExtractor, self).__init__()\n",
    "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
    "                               int(output_channel / 2), output_channel]  # [64, 128, 256, 512]\n",
    "        self.ConvNet = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # 64x16x50\n",
    "            nn.Conv2d(self.output_channel[0], self.output_channel[1], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # 128x8x25\n",
    "            nn.Conv2d(self.output_channel[1], self.output_channel[2], 3, 1, 1), nn.ReLU(True),  # 256x8x25\n",
    "            nn.Conv2d(self.output_channel[2], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # 256x4x25\n",
    "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),  # 512x4x25\n",
    "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # 512x2x25\n",
    "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 2, 1, 0), nn.ReLU(True))  # 512x1x24\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.ConvNet(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.879206Z",
     "iopub.status.busy": "2023-05-02T18:01:13.878941Z",
     "iopub.status.idle": "2023-05-02T18:01:13.885499Z",
     "shell.execute_reply": "2023-05-02T18:01:13.884766Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.879181Z"
    }
   },
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input : visual feature [batch_size x T x input_size]\n",
    "        output : contextual feature [batch_size x T x output_size]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.rnn.flatten_parameters()\n",
    "        except:\n",
    "            pass\n",
    "        recurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n",
    "        output = self.linear(recurrent)  # batch_size x T x output_size\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.886488Z",
     "iopub.status.busy": "2023-05-02T18:01:13.886257Z",
     "iopub.status.idle": "2023-05-02T18:01:13.891762Z",
     "shell.execute_reply": "2023-05-02T18:01:13.890952Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.886464Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    print(\"Modules, Parameters\")\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        #table.add_row([name, param])\n",
    "        total_params+=param\n",
    "        print(name, param)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T18:01:13.893493Z",
     "iopub.status.busy": "2023-05-02T18:01:13.893255Z",
     "iopub.status.idle": "2023-05-02T18:01:14.194143Z",
     "shell.execute_reply": "2023-05-02T18:01:14.191245Z",
     "shell.execute_reply.started": "2023-05-02T18:01:13.893469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: all_data\n",
      "opt.select_data: ['en_train_filtered']\n",
      "opt.batch_ratio: ['1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    all_data\t dataset: en_train_filtered\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'all_data/en_train_filtered/.ipynb_checkpoints/labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [429], line 287\u001b[0m\n\u001b[1;32m    281\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    286\u001b[0m opt \u001b[38;5;241m=\u001b[39m get_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_files/en_filtered_config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [429], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(opt, show_number, amp)\u001b[0m\n\u001b[1;32m      7\u001b[0m opt\u001b[38;5;241m.\u001b[39mselect_data \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mselect_data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m opt\u001b[38;5;241m.\u001b[39mbatch_ratio \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mbatch_ratio\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mBatch_Balanced_Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./saved_models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt\u001b[38;5;241m.\u001b[39mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/log_dataset.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m AlignCollate_valid \u001b[38;5;241m=\u001b[39m AlignCollate(imgH\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mimgH, imgW\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mimgW, keep_ratio_with_pad\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mPAD, contrast_adjust\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mcontrast_adjust)\n",
      "Cell \u001b[0;32mIn [419], line 26\u001b[0m, in \u001b[0;36mBatch_Balanced_Dataset.__init__\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(dashed_line)\n\u001b[1;32m     25\u001b[0m log\u001b[38;5;241m.\u001b[39mwrite(dashed_line \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m _dataset, _dataset_log \u001b[38;5;241m=\u001b[39m \u001b[43mhierarchical_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselect_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mselected_d\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m total_number_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(_dataset)\n\u001b[1;32m     28\u001b[0m log\u001b[38;5;241m.\u001b[39mwrite(_dataset_log)\n",
      "Cell \u001b[0;32mIn [417], line 16\u001b[0m, in \u001b[0;36mhierarchical_dataset\u001b[0;34m(root, opt, select_data)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m select_flag:\n\u001b[0;32m---> 16\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mOCRDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     sub_dataset_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-directory:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrelpath(dirpath,\u001b[38;5;250m \u001b[39mroot)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m num samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sub_dataset_log)\n",
      "Cell \u001b[0;32mIn [425], line 10\u001b[0m, in \u001b[0;36mOCRDataset.__init__\u001b[0;34m(self, root, opt)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt \u001b[38;5;241m=\u001b[39m opt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#root = glob.glob(\"all_data/en_train_filtered/[!.]**.csv\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#root = root[0]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print('root: ',root)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m^([^,]+),\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#self.df = pd.read_csv(root, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnSamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     is_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    856\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'all_data/en_train_filtered/.ipynb_checkpoints/labels.csv'"
     ]
    }
   ],
   "source": [
    "def train(opt, show_number = 2, amp=False):\n",
    "    \"\"\" dataset preparation \"\"\"\n",
    "    if not opt.data_filtering_off:\n",
    "        print('Filtering the images containing characters which are not in opt.character')\n",
    "        print('Filtering the images whose label is longer than opt.batch_max_length')\n",
    "\n",
    "    opt.select_data = opt.select_data.split('-')\n",
    "    opt.batch_ratio = opt.batch_ratio.split('-')\n",
    "    train_dataset = Batch_Balanced_Dataset(opt)\n",
    "\n",
    "    log = open(f'./saved_models/{opt.experiment_name}/log_dataset.txt', 'a', encoding=\"utf8\")\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD, contrast_adjust=opt.contrast_adjust)\n",
    "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=min(32, opt.batch_size),\n",
    "        shuffle=True,  # 'True' to check training progress with validation function.\n",
    "        num_workers=int(opt.workers), prefetch_factor=512,\n",
    "        collate_fn=AlignCollate_valid, pin_memory=True)\n",
    "    log.write(valid_dataset_log)\n",
    "    print('-' * 80)\n",
    "    log.write('-' * 80 + '\\n')\n",
    "    log.close()\n",
    "    \n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "\n",
    "    if opt.saved_model != '':\n",
    "        #check from here\n",
    "        #print(\"checking if this condition runs!!\")\n",
    "        pretrained_dict = torch.load(opt.saved_model)\n",
    "        if opt.new_prediction:\n",
    "            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))  \n",
    "        \n",
    "        model = torch.nn.DataParallel(model).to(device) \n",
    "        print(f'loading pretrained model from {opt.saved_model}')\n",
    "        if opt.FT:\n",
    "            model.load_state_dict(pretrained_dict, strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(pretrained_dict)\n",
    "        if opt.new_prediction:\n",
    "            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)  \n",
    "            for name, param in model.module.Prediction.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            model = model.to(device) \n",
    "    else:\n",
    "        # weight initialization\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'localization_fc2' in name:\n",
    "                print(f'Skip {name} as it is already initialized')\n",
    "                continue\n",
    "            try:\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            except Exception as e:  # for batchnorm.\n",
    "                if 'weight' in name:\n",
    "                    param.data.fill_(1)\n",
    "                continue\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "    \n",
    "    model.train() \n",
    "    print(\"Model:\")\n",
    "    print(model)\n",
    "    count_parameters(model)\n",
    "    \n",
    "    \"\"\" setup loss \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n",
    "    # loss averager\n",
    "    loss_avg = Averager()\n",
    "\n",
    "    # freeze some layers\n",
    "    try:\n",
    "        if opt.freeze_FeatureFxtraction:\n",
    "            for param in model.module.FeatureExtraction.parameters():\n",
    "                param.requires_grad = False\n",
    "        if opt.freeze_SequenceModeling:\n",
    "            for param in model.module.SequenceModeling.parameters():\n",
    "                param.requires_grad = False\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # filter that only require gradient decent\n",
    "    filtered_parameters = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        filtered_parameters.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "    print('Trainable params num : ', sum(params_num))\n",
    "    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n",
    "\n",
    "    # setup optimizer\n",
    "    if opt.optim=='adam':\n",
    "        #optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "        optimizer = optim.Adam(filtered_parameters)\n",
    "    else:\n",
    "        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
    "    print(\"Optimizer:\")\n",
    "    print(optimizer)\n",
    "\n",
    "    \"\"\" final options \"\"\"\n",
    "    # print(opt)\n",
    "    with open(f'./saved_models/{opt.experiment_name}/opt.txt', 'a', encoding=\"utf8\") as opt_file:\n",
    "        opt_log = '------------ Options -------------\\n'\n",
    "        args = vars(opt)\n",
    "        for k, v in args.items():\n",
    "            opt_log += f'{str(k)}: {str(v)}\\n'\n",
    "        opt_log += '---------------------------------------\\n'\n",
    "        print(opt_log)\n",
    "        opt_file.write(opt_log)\n",
    "\n",
    "    \"\"\" start training \"\"\"\n",
    "    start_iter = 0\n",
    "    if opt.saved_model != '':\n",
    "        try:\n",
    "            start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n",
    "            print(f'continue to train, start_iter: {start_iter}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "    i = start_iter\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    t1= time.time()\n",
    "        \n",
    "    while(True):\n",
    "        # train part\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if amp:\n",
    "            with autocast():\n",
    "                image_tensors, labels = train_dataset.get_batch()\n",
    "                image = image_tensors.to(device)\n",
    "                text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "                batch_size = image.size(0)\n",
    "\n",
    "                if 'CTC' in opt.Prediction:\n",
    "                    preds = model(image, text).log_softmax(2)\n",
    "                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                    preds = preds.permute(1, 0, 2)\n",
    "                    torch.backends.cudnn.enabled = False\n",
    "                    cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                    torch.backends.cudnn.enabled = True\n",
    "                else:\n",
    "                    preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                    target = text[:, 1:]  # without [GO] Symbol\n",
    "                    cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            scaler.scale(cost).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            image_tensors, labels = train_dataset.get_batch()\n",
    "            image = image_tensors.to(device)\n",
    "            text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "            batch_size = image.size(0)\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text).log_softmax(2)\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                preds = preds.permute(1, 0, 2)\n",
    "                torch.backends.cudnn.enabled = False\n",
    "                cost = criterion(preds, text.to(device), preds_size.to(device), length.to(device))\n",
    "                torch.backends.cudnn.enabled = True\n",
    "            else:\n",
    "                preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "                target = text[:, 1:]  # without [GO] Symbol\n",
    "                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "            cost.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip) \n",
    "            optimizer.step()\n",
    "        loss_avg.add(cost)\n",
    "\n",
    "        # validation part\n",
    "        if (i % opt.valInterval == 0) and (i!=0):\n",
    "            print('training time: ', time.time()-t1)\n",
    "            t1=time.time()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            # for log\n",
    "            with open(f'./saved_models/{opt.experiment_name}/log_train.txt', 'a', encoding=\"utf8\") as log:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    #valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels, \\\n",
    "                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels, infer_time = validation(model, criterion, valid_loader, converter, opt, device)\n",
    "                    #infer_time, length_of_data = validation(model, criterion, valid_loader, converter, opt, device)\n",
    "                model.train()\n",
    "\n",
    "                # training loss and validation loss\n",
    "                \n",
    "                #print(\"check type -> loss_avg: {}\".format(loss_avg.val()))\n",
    "                #print(\"check type -> valid_loss: {}\".format(valid_loss))\n",
    "                #print(\"check type -> elapsed_time: {}\".format(elapsed_time))\n",
    "                \n",
    "                loss_average = loss_avg.val()\n",
    "                default_value = 0\n",
    "\n",
    "                if loss_average is None:\n",
    "                    loss_average = default_value\n",
    "                \n",
    "                if valid_loss is None:\n",
    "                    valid_loss = default_value\n",
    "                    \n",
    "                if elapsed_time is None:\n",
    "                    elapsed_time = default_value\n",
    "                \n",
    "                \n",
    "                loss_log = f'[{i}/{opt.num_iter}] Train loss: {loss_average:0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n",
    "                loss_avg.reset()\n",
    "\n",
    "                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.4f}'\n",
    "\n",
    "                # keep best accuracy model (on valid dataset)\n",
    "                if current_accuracy > best_accuracy:\n",
    "                    best_accuracy = current_accuracy\n",
    "                    torch.save(model.state_dict(), f'./saved_models/{opt.experiment_name}/best_accuracy.pth')\n",
    "                if current_norm_ED > best_norm_ED:\n",
    "                    best_norm_ED = current_norm_ED\n",
    "                    torch.save(model.state_dict(), f'./saved_models/{opt.experiment_name}/best_norm_ED.pth')\n",
    "                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.4f}'\n",
    "\n",
    "                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                print(loss_model_log)\n",
    "                log.write(loss_model_log + '\\n')\n",
    "\n",
    "                # show some predicted results\n",
    "                dashed_line = '-' * 80\n",
    "                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n",
    "                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "                \n",
    "                #show_number = min(show_number, len(labels))\n",
    "                \n",
    "                \n",
    "                #print(\"check -> labels: {}\".format(labels))\n",
    "                #print(\"check -> show_number: {}\".format(show_number))\n",
    "                \n",
    "                start = random.randint(0,len(labels) - show_number )    \n",
    "                \n",
    "                for gt, pred, confidence in zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number]):\n",
    "                #list_zip = zip(labels[start:start+show_number], preds[start:start+show_number], confidence_score[start:start+show_number])\n",
    "                 \n",
    "                #for gt, pred, confidence in list_zip: \n",
    "                    \n",
    "                    if 'Attn' in opt.Prediction:\n",
    "                        gt = gt[:gt.find('[s]')]\n",
    "                        pred = pred[:pred.find('[s]')]\n",
    "\n",
    "                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
    "                predicted_result_log += f'{dashed_line}'\n",
    "                print(predicted_result_log)\n",
    "                log.write(predicted_result_log + '\\n')\n",
    "                print('validation time: ', time.time()-t1)\n",
    "                t1=time.time()\n",
    "        # save model per 1e+4 iter.\n",
    "        if (i + 1) % 1e+4 == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(), f'./saved_models/{opt.experiment_name}/iter_{i+1}.pth')\n",
    "\n",
    "        if i == opt.num_iter:\n",
    "            print('end the training')\n",
    "            #sys.exit()\n",
    "        i += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "opt = get_config(\"config_files/en_filtered_config.yaml\")\n",
    "train(opt, amp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-02T18:01:14.194922Z",
     "iopub.status.idle": "2023-05-02T18:01:14.195296Z",
     "shell.execute_reply": "2023-05-02T18:01:14.195108Z",
     "shell.execute_reply.started": "2023-05-02T18:01:14.195090Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "os.path.isfile('config_files/en_filtered_config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------- Testing -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-02T18:01:14.196735Z",
     "iopub.status.idle": "2023-05-02T18:01:14.197045Z",
     "shell.execute_reply": "2023-05-02T18:01:14.196907Z",
     "shell.execute_reply.started": "2023-05-02T18:01:14.196890Z"
    }
   },
   "outputs": [],
   "source": [
    "class load_dict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(load_dict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "\n",
    "def load_files(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
    "        opt = yaml.safe_load(stream)\n",
    "    opt = load_dict(opt)\n",
    "    return opt\n",
    "\n",
    "opt = load_files(\"my_model/my_model.yaml\")\n",
    "opt.num_class = len(opt.character_list)\n",
    "\n",
    "\n",
    "from my_model.my_model import Model\n",
    "trained_model = Model(opt.input_channel, opt.output_channel, opt.hidden_size, opt.num_class)\n",
    "#pretrained_dict = torch.load('/notebooks/saved_models/en_filtered/best_accuracy.pth')\n",
    "pretrained_dict = torch.load(opt.my_model)\n",
    "\n",
    "trained_model = torch.nn.DataParallel(trained_model).to(device) \n",
    "print(f'loading pretrained model from {opt.my_model}')\n",
    "\n",
    "trained_model.load_state_dict(pretrained_dict, strict=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-02T18:01:14.198162Z",
     "iopub.status.idle": "2023-05-02T18:01:14.198535Z",
     "shell.execute_reply": "2023-05-02T18:01:14.198381Z",
     "shell.execute_reply.started": "2023-05-02T18:01:14.198353Z"
    }
   },
   "outputs": [],
   "source": [
    " pretrained_dict = torch.load(opt.saved_model)\n",
    "        if opt.new_prediction:\n",
    "            model.Prediction = nn.Linear(model.SequenceModeling_output, len(pretrained_dict['module.Prediction.weight']))  \n",
    "        \n",
    "        model = torch.nn.DataParallel(model).to(device) \n",
    "        print(f'loading pretrained model from {opt.saved_model}')\n",
    "        if opt.FT:\n",
    "            model.load_state_dict(pretrained_dict, strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(pretrained_dict)\n",
    "        if opt.new_prediction:\n",
    "            model.module.Prediction = nn.Linear(model.module.SequenceModeling_output, opt.num_class)  \n",
    "            for name, param in model.module.Prediction.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    init.kaiming_normal_(param)\n",
    "            model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-02T18:01:14.200144Z",
     "iopub.status.idle": "2023-05-02T18:01:14.200453Z",
     "shell.execute_reply": "2023-05-02T18:01:14.200316Z",
     "shell.execute_reply.started": "2023-05-02T18:01:14.200298Z"
    }
   },
   "outputs": [],
   "source": [
    "rm -r .ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
